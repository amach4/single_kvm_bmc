{#
#
# (c) Copyright 2015-2016 Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
#}

[DEFAULT]

# Print debugging output (set logging level to DEBUG instead
# of default WARNING level). (boolean value)
debug={{ cinder_debug }}

enable_v3_api=True

osapi_volume_listen = {{ cinder_osapi_volume_listen }}
osapi_volume_listen_port = {{ cinder_osapi_volume_listen_port }}
bind_host = {{ cinder_bind_host }}
{% if cinder_glance_url is defined %}
glance_api_servers = {{ cinder_glance_url }}
{% endif %}
{% if cinder_backup_swift_url is defined %}
backup_swift_url = {{ cinder_backup_swift_url }}
swift_catalog_info = object-store:swift:internalURL
{% endif %}
{% if cinder_volume_nova_url is defined %}
nova_endpoint_template = {{ cinder_volume_nova_url }}/%(project_id)s
nova_endpoint_admin_template = {{ cinder_volume_nova_url }}/%(project_id)s
os_region_name = {{ cinder_keystone.region_name }}
nova_catalog_info = compute:nova:internalURL
nova_catalog_admin_info = compute:nova:adminURL
{% endif %}

{% block api_paste_config %}
# Versioned api-paste.ini specified where appropriate.
#api_paste_config = /etc/cinder/api-paste.ini
{% endblock api_paste_config %}

auth_strategy = keystone

state_path = {{ cinder_state_path }}
{% if cinder_image_conversion_dir is defined %}
image_conversion_dir={{ cinder_image_conversion_dir }}
{% endif %}

iscsi_helper={{ cinder_iscsi_helper }}


rpc_backend=cinder.openstack.common.rpc.impl_kombu

{% if cinder_public_endpoint is defined %}
# Uncomment to set the public API in the links responses
#public_endpoint = {{ cinder_public_endpoint }}
#osapi_volume_base_URL = {{ cinder_public_endpoint }}
{% endif %}
# behind haproxy record the X-Forward-For source in the logfile
use_forwarded_for = True


control_exchange = {{ cinder_control_exchange }}
rpc_response_timeout = 120

# Common hostname to avoid singleton limitation of Cinder volume manager
host = ha-volume-manager

{% if ((cinderinternal_project_id is defined) and (cinderinternal_user_id is defined)) %}
# Cinder internal project id and user id for volume image cache
cinder_internal_tenant_project_id = {{ cinderinternal_project_id }}
cinder_internal_tenant_user_id = {{ cinderinternal_user_id }}
# To enable volume image caching the image_volume_cache_enabled flag needs to
# be set to True in the relevant backend section in cinder.conf
{% endif %}

{% if my_enabled_backends.cinder_enabled_backends is not defined %}

# Configure the enabled backends
enabled_backends=ceph1
{%- if cinder_lvm_device_group %},lvm-1{% endif %}

{% endif %}

# Configure the enabled backends, above left for convenience, until upgrade
# process is finalised.
{% if my_enabled_backends.cinder_enabled_backends is defined %}
enabled_backends={{ my_enabled_backends.cinder_enabled_backends[inventory_hostname]|join(", ") }}

{% for be in my_enabled_backends.cinder_backend_configs %}
[{{ be['name'] }}]
{% for conf in be['config'] %}
{{ conf }}={{ be['config'][conf] }}
{% endfor %}

{% endfor %}
{% endif %}

{% if cinder_keymgr_fixed_key is defined %}
[keymgr]
fixed_key = {{ cinder_keymgr_fixed_key }}
{% elif cinder_keymgr_url is defined and cinder_keymgr_url != "" %}
[keymgr]
api_class = cinder.keymgr.barbican.BarbicanKeyManager
encryption_auth_url = {{ cinder_keystone.internal_url }}/v3
encryption_api_url = {{ cinder_keymgr_url }}/v1
{% endif %}

[keystone_authtoken]
auth_uri = {{ cinder_keystone.identity_admin_url }}

[database]
connection={{ cinder_db }}
max_overflow={{ cinder_max_overflow }}
max_pool_size={{ cinder_max_pool_size }}
min_pool_size={{ cinder_min_pool_size }}

[oslo_messaging_notifications]
driver = {{ cinder_notification_driver }}

[oslo_messaging_rabbit]
rabbit_ha_queues=False
rabbit_hosts = {{ cinder_rabbit_hosts }}
rabbit_password = {{ cinder_rabbit_password }}
rabbit_use_ssl = {{ cinder_rabbit_use_ssl }}
rabbit_userid = {{ cinder_rabbit_userid }}

[oslo_middleware]
enable_proxy_headers_parsing = true

[oslo_concurrency]
lock_path = /var/lib/cinder

{% block audit_middleware %}{% endblock audit_middleware %}
{% if cinder_lvm_device_group %}
# LVM thin provision. This way we don't dd the disk
[lvm-1]
volume-group = cinder-volumes
lvm_type = thin
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_backend_name = LVM_iSCSI
{% endif %}

# Start of section for StoreVirtual (VSA) cluster
#
# If you have configured StoreVirtual backend storage for cinder you
# must uncomment this section, and replace all strings in angle
# brackets with the correct values for the cluster you have
# configured.  You must also add the section name to the list of
# values in the 'enabled_backends' variable above.
#
# If you have more than one StoreVirtual cluster you must provide this
# whole section for each cluster and provide a unique section name for
# each cluster. For example, replace <unique-section-name> with
# VSA_CLUSTER_1 for one cluster and VSA_CLUSTER_2 for the other.
#
# Note, if you want to group more than one cluster under a single
# cinder backend then you can give the same value to <vsa-backend-name>
# in sections for several clusters.
#
# Use this model if the StoreVirtual Storage array is running LeftHand OS
# version 11 or higer
#
#[<unique-section-name>]
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
#hplefthand_password = {{ '<encrypted vsa-cluster-password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
#hplefthand_password = <vsa-cluster-password>
#hplefthand_clustername = <vsa-cluster-name>
#hplefthand_api_url = https://<vsa-cluster-vip>:8081/lhos
#hplefthand_username =  <vsa-cluster-username>
#hplefthand_iscsi_chap_enabled = true
#volume_backend_name = <vsa-backend-name>
#volume_driver = cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
#hplefthand_debug = false
#
#
# Use this model if the StoreVirtual Storage array is running LeftHand OS
# lower than version 11
#
# [<unique-section-name>]
# volume_driver=cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
# volume_backend_name=lefthand-cliq
# san_ip=<san-ip>
# san_login=<san_username>
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
# san_password= {{ '<encrypted san_password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
# san_password=<san_password>
# san_ssh_port=16022
# san_clustername=<vsa-cluster-name>
#volume_backend_name = <vsa-backend-name>
#
#
# End of section for StoreVirtual (VSA) cluster

# Start of section for StoreServ (3par) iscsi cluster
#
# If you have configured StoreServ backend storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for the HP 3PAR you have configured.  You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above. You must provide unique section
# each time you configure a new backend for HP 3PAR.
#
# If you want to configure more than one CPG then you can do one of the
# following:
#   1) Create an unique section for each backend, or
#   2) Provide a comma separated list of CPGs for hp3par_cpg
#
# In the second case, this set of CPGs will form a pool but will be seen as a
# single device by Cinder.
#
#[<unique-section-name>]
#hp3par_iscsi_chap_enabled = true
#san_ip = <3par-san-ipaddr>
#san_login = <3par-san-username>
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
# san_password = {{ '<encrypted 3par-san-password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
# san_password = <3par-san-password>
#hp3par_iscsi_ips = <3par-ip-address-1>[,<3par-ip-address-2>,<3par-ip-address-3>, ...]
#hp3par_username = <3par-username>
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
#hp3par_password = {{ '<encrypted hp3par_password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
#hp3par_password = <hp3par_password>
#hp3par_api_url = https://<3par-san-ipaddr>:8080/api/v1
#hp3par_cpg = <3par-cpg-name-1>[,<3par-cpg-name-2>, ...]
#volume_backend_name = <3par-backend-name>
#volume_driver = cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
#
# End of section for StoreServ (3par) iscsi cluster


# Start of section to enable a CPG  as a backend for StoreServ (3par)
# using a fibre channel device
#
# If you have configured StoreServ backend storage for cinder you must
# uncomment this section, and replace all strings in angle brackets
# with the correct values for the cluster you have configured.  You
# must also add the section name to the list of values in the
# 'enabled_backends' variable above.
#
# If you want to configure more than one CPG then you can do one of the
# following:
#   1) Create an unique section for each backend, or
#   2) Provide a comma separated list of CPGs for hp3par_cpg
#
# In second case, these sets of CPGs will form a pool but will seen as a
# single device by Cinder.
#
#
#[<unique-section-name>]
#san_ip = <3par-san-ipaddr>
#san_login = <3par-san-username>
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
#san_password = {{ '<encrypted 3par-san-password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
#san_password = <3par-san-password>
#hp3par_username = <3par-username>
# If adding a password here, then the password can be encrypted using the
# mechanism specified in the documentation.  If the password has been encrypted
# add the value and the hos_user_password_decrypt filter like so:
#hp3par_password = {{ '<encrypted 3par-password>' | hos_user_password_decrypt }}
# Note that the encrypted value has to be enclosed in quotes
# If you choose not to encrypt the password then the unencrypted password
# must be set as follows:
#hp3par_password = <3par-password>
#hp3par_api_url = https://<3par-san-ipaddr>:8080/api/v1
#hp3par_cpg = <3par-cpg-name-1>[,<3par-cpg-name-2>,...]
#volume_backend_name = <3par-backend-name>
#volume_driver = cinder.volume.drivers.san.hp.hp_3par_fc.HP3PARFCDriver
#
# End of section for StoreServ (3par) fibre channel cluster

# Start of section for ceph
#
# If you have configured ceph backend storage for cinder you
# must uncomment this section, and replace all strings in angle
# brackets with the correct values for the ceph you have
# configured.  You must also add the section name to the list of
# values in the 'enabled_backends' variable above.
#
# If you have more than one ceph backend you must provide this
# whole section for each ceph backend and provide a unique section name for
# each. For example, replace <unique-section-name> with CEPH_1 for
# one backend and CEPH_2 for the other.
#
#[<unique-section-name>]
#rbd_secret_uuid = <secret-uuid>
#rbd_user = <ceph-cinder-user>
#rbd_pool = <ceph-cinder-volume-pool>
#rbd_ceph_conf = <ceph-config-file>
#volume_driver = cinder.volume.drivers.rbd.RBDDriver
#volume_backend_name = <ceph-backend-name>
#
[ceph1]
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337
rbd_user = cinder
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_cluster_name = ceph
volume_driver = cinder.volume.drivers.rbd.RBDDriver
volume_backend_name = ceph1
# End of section for ceph

# Start of section for vmdk
#
# If you have configured vmdk storage for cinder you
# must uncomment this section, and replace all strings in angle
# brackets with the correct values for the cluster you have
# configured.  You must also add the section name to the list of
# values in the 'enabled_backends' variable above.
#
# If you have more than one vmdk backend you must provide this
# whole section for each vmdk backend and provide a unique section name for
# each. For example, replace <unique-section-name> with
# VMDK_1 for one backend and VMDK_2 for the other.
#
#[<unique-section-name>]
#vmware_host_ip = <vmware-host-ip>
#vmware_host_password = <vmware-host-password>
#vmware_host_username = <vmware-host-username>
#vmware_insecure = True
#volume_driver = cinder.volume.drivers.vmware.vmdk.VMwareVcVmdkDriver
#volume_backend_name=<vmdk-backend-name>
#
# End of section for vmdk

# Start of section for Broccade Fiber channel Zone Manager
#
# If you have configured Fibre Channel Volume Driver that supports Zone Manager for cinder you
# must uncomment this section, and replace all strings in angle
# brackets with the correct values you have configured.
# In the below configuration fc_fabric_names can be mutilple names,
# you have to define seperate section for each name with appropriate switch details

#[DEFAULT]
#zoning_mode=fabric

#[fc-zone-manager]
#brcd_sb_connector = cinder.zonemanager.drivers.brocade.brcd_fc_zone_client_cli.BrcdFCZoneClientCLI
#fc_san_lookup_service = cinder.zonemanager.drivers.brocade.brcd_fc_san_lookup_service.BrcdFCSanLookupService
#zone_driver = cinder.zonemanager.drivers.brocade.brcd_fc_zone_driver.BrcdFCZoneDriver
#fc_fabric_names = <unique-fabric-name>

#[<unique-fabric-name>]
#fc_fabric_address = <switch-ip-address>
#fc_fabric_user = <switch-user-name>
#fc_fabric_password = <switch-password>
#principal_switch_wwn = <switch-wwn>
#zoning_policy = initiator-target
#zone_activate = true
#zone_name_prefix = <zone-name-prefix>

# End of section for Broccade Fiber channel Zone Manager

# Start of section for ceph backup
#
# If you have configured ceph backup storage for cinder you
# must uncomment this section, and replace all strings in angle
# brackets with the correct values for the ceph you have
# configured.
#
[DEFAULT]
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_pool = backups
#
# End of section for ceph backup
