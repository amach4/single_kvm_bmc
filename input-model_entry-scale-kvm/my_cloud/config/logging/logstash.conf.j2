{#
#
# (c) Copyright 2015,2016 Hewlett Packard Enterprise Development LP
# (c) Copyright 2017 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.
#
#}

# Logstash inputs
#----------------------------------------------------------------------------------------
input {

  # Kafka input source
  # Primary input for all Ardana OpenStack components
  # https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html
  kafka {
    # Kafka topic to consume messages from
    topic_id => "{{ kronos_kafka_topic }}"

    # ZooKeeper comma delimeted connections strings
    zk_connect => "{{ kronos_zookeeper_hosts }}"

    # A string that uniquely identifies the group of consumer processes to which this consumer
    # belongs. By setting the same group id multiple processes indicate that they are all part of
    # the same consumer group.
    group_id => "{{ kronos_kafka_consumer_group }}"

    # Number of threads to read from the partitions. Ideally you should have as many threads as
    # the number of partitions for a perfect balance.
    consumer_threads => {{ logstash_threads }}

    # Retry when an error is received
    consumer_restart_on_error => true
  }
}

# Logstash filters
#----------------------------------------------------------------------------------------
filter {

  # This must be first for the other filters to complete successfully.
  # Unwrap the envelope from the log message.
  ruby {
    code => "
     ['log', 'meta', 'dimensions'].each { |item|
        next if not event.include? item
        event[item].each {|k, v|
          next if k=='@timestamp'
          if k=='message' and item=='log' and v.instance_of? Hash
              v.each { |i,j|
                  next if i=='@timestamp'
                  event[i] = j
              }
              event[item].delete(k)
          elsif item == 'meta'
              event['log_api_'.concat(k)] = v
          else
              event[k] = v
          end
        }
        event.remove(item)
     }
    "
  }

  # Filter out auth tokens
  if [auth_token]{
    anonymize {
      algorithm => 'SHA1'
      fields    => ['auth_token']
      key       => '{{ logstash_anonymize_salt }}'
    }
    mutate {
      replace => [ 'auth_token', '{SHA1}%{auth_token}' ]
    }
  }

  # Normalize dates to ISO8601
  # Services like neutron may log a timestamp field that uses a format not normally
  # recognized by Elasticsearch. This may cause ES to map the same field as string
  # for one service and date for another service - resulting in a mapping conflict
  # at Kibana. The following filter recognizes both formats and normalize them to
  # ISO8601. If the timestamp field causes the date parsing to fail, the field is
  # dropped silently.
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "YYYY-MM-dd HH:mm:ss.SSSSSS"]
      target => "timestamp"
    }
    mutate {
      rename => { "timestamp" => "event_timestamp" }
    }
    if "_dateparsefailure" in [tags] {
      mutate {
        remove_field => "event_timestamp"
        remove_tag => ["_dateparsefailure" ]
      }
      mutate {
        add_field => { "[event_timestamp]" => "%{@timestamp}" }
      }
    }
  } else {
    mutate {
      add_field => { "[event_timestamp]" => "%{@timestamp}" }
    }
  }

  # Ensure that the 'service' field will always exist in the case where
  # something other than beaver pushes the logs to the API.
  if ![service] {
    if [type] {
      mutate { add_field => { "service" => "%{type}" } }
    } else {
      mutate { add_field => { "service" => "other" } }
    }
  }

  # Decide what ES index the log should go into based on the 'log_type' and 'service'
  # Also ensure that each type has it's own index to avoid mapping conflicts going forward
  if "audit" in [log_type] {
    mutate { add_field => { "[@metadata][es_index]" => "{{ logstash_audit_logs_prefix }}%{service}-%{+YYYY.MM.dd}" } }
    mutate { remove_field => "log_type" }
  } else {
    mutate { add_field => { "[@metadata][es_index]" => "{{ logstash_operational_logs_prefix }}%{service}-%{+YYYY.MM.dd}" } }
  }

}

# Logstash outputs
#----------------------------------------------------------------------------------------
output {

  # Configure Elasticsearch output
  # http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html
  elasticsearch {
    index => "%{[@metadata][es_index]}"
    hosts => ["{{ elasticsearch_http_host }}:{{ elasticsearch_http_port }}"]
    flush_size => {{ logstash_flush_size }}
    idle_flush_time => 5
    workers => {{ logstash_threads }}
  }
}
